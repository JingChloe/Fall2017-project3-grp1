---
title: "Project 3 - Example Main Script"
author: "Grp 1"
date: "Oct 21, 2017"
output:
  html_document: default
  pdf_document: default
---

```{r, warning=F}
if(!require("EBImage")){
  source("https://bioconductor.org/biocLite.R")
  biocLite("EBImage")
}

if(!require("gbm")){
  install.packages("gbm")
}

library("EBImage")
library("gbm")
library("MASS")
```

### Step 0: specify directories.

```{r, warning=F}
experiment_dir <- "../data/training_set/" # This will be modified for different data sets.
img_train_dir <- paste(experiment_dir, "train/", sep="")
img_test_dir <- paste(experiment_dir, "test/", sep="")
```

### Step 1: set up controls for evaluation experiments.

In this chunk, ,we have a set of controls for the evaluation experiments. 

+ (T/F) cross-validation on the training set
+ (number) K, the number of CV folds
+ (T/F) process features for training set
+ (T/F) run evaluation on an independent test set
+ (T/F) process features for test set

```{r exp_setup, warning=F}
run.cv = FALSE # run cross-validation on the training set
K <- 5  # number of CV folds
run.feature.train = TRUE # process features for training set
run.test = TRUE # run evaluation on an independent test set
run.feature.test = TRUE # process features for test set

run.pca = TRUE #Performs PCA dimensionality reduction on images
run.hogs = TRUE
```

Using cross-validation or independent test set evaluation, we compare the performance of different classifiers or classifiers with different specifications. In this example, we use GBM with different `depth`. In the following chunk, we list, in a vector, setups (in this case, `depth`) corresponding to models that we will compare. In your project, you maybe comparing very different classifiers. You can assign them numerical IDs and labels specific to your project. 

```{r model_setup, warning=F}
model_values <- seq(4, 8, 2)
model_labels = paste("GBM with depth =", model_values)
```

### Step 2: import training images class labels.

```{r train_label, warning=F}
label_train <- read.csv("../data/training_set/label_train.csv", header=T)
```

### Step 3: construct visual feature

```{r feature, warning=F}
source("../lib/feature_PCA.R")

tm_feature_train <- NA
if(run.feature.train){
  tm_feature_train <- system.time(dat_train <- feature(img_train_dir,
                                                       n_pixel_row = 50,
                                                       n_pixel_col = 50,
                                                       n_dig = 4,
                                                       n_hogs = 54,
                                                       desired_variance = 0.9,
                                                       run.pca = run.pca,
                                                       run.hogs = run.hogs,
                                                       export=TRUE))
}

```

### Step 4: Train a classification model with training images
Call the train model and test model from library. 

`train.R` and `test.R` should be wrappers for all your model training steps and your classification/prediction steps. 
+ `train.R`
  + Input: a path that points to the training set features.
  + Input: an R object of training sample labels.
  + Output: an RData file that contains trained classifiers in the forms of R objects: models/settings/links to external trained configurations.
+ `test.R`
  + Input: a path that points to the test set features.
  + Input: an R object that contains a trained classifier.
  + Output: an R object of class label predictions on the test set. If there are multiple classifiers under evaluation, there should be multiple sets of label predictions. 
```{r loadlib, warning=F}
source("../lib/train.R")
source("../lib/test.R")


```

LDA and QDA Analysis

```{r}
train.qda <- qda(x = dat_train$HPCA, grouping = label_train[,2], method = "mle", CV = T)
error1    <- sum(train.qda$class != label_train[,2])/length(label_train[,2])

train.lda <- lda(x = dat_train$HPCA, grouping = label_train[,2], method = "mle", CV = T)
error2    <- sum(train.lda$class != label_train[,2])/length(label_train[,2])
```




#### Model selection with cross-validation
* Do model selection by choosing among different values of training model parameters, that is, the interaction depth for GBM in this example. 
```{r runcv, message=FALSE, warning=FALSE}
source("../lib/cross_validation.R")
dat_train <- read.csv("../data/training_set/sift_train.csv", header=T)
dat_train <- dat_train[,-1]

selected <- sample(size = 100, nrow(dat_train))
cv.dat_train <- dat_train[selected,]
cv.label_train <- label_train[selected,]

if(run.cv){
  err_cv <- array(dim=c(length(model_values), 2))
  for(k in 1:length(model_values)){
    cat("k=", k, "\n")
    err_cv[k,] <- cv.function(cv.dat_train, cv.label_train[,2], model_values[k], K)
  }
  save(err_cv, file="../output/err_cv.RData")
}
```

Visualize cross-validation results. 

```{r cv_vis, warning=F}
if(run.cv){
  load("../output/err_cv.RData")
  #pdf("../fig/cv_results.pdf", width=7, height=5)
  plot(model_values, err_cv[,1], xlab="Interaction Depth", ylab="CV Error",
       main="Cross Validation Error", type="n", ylim=c(0, 1))
  points(model_values, err_cv[,1], col="blue", pch=16)
  lines(model_values, err_cv[,1], col="blue")
  arrows(model_values, err_cv[,1]-err_cv[,2], model_values, err_cv[,1]+err_cv[,2], 
        length=0.1, angle=90, code=3)
  #dev.off()
}
```


* Choose the "best"" parameter value
```{r best_model, warning=F}
model_best=model_values[1]
if(run.cv){
  model_best <- model_values[which.min(err_cv[,1])]
}

par_best <- list(depth=model_best)
```

* Train the model with the entire training set using the selected model (model parameter) via cross-validation.
```{r final_train, warning=F}
tm_train=NA
tm_train <- system.time(fit_train <- train(dat_train, label_train[,2], par_best))
save(fit_train, file="../output/fit_train.RData")
```

### Step 5: Make prediction 
 


### Summarize Running Time

Prediction performance matters, so does the running times for constructing features and for training the model, especially when the computation resource is limited. 
```{r running_time, warning=F}
cat("Time for constructing training features=", tm_feature_train[1], "s \n")

cat("Time for training model=", tm_train[1], "s \n")
```
