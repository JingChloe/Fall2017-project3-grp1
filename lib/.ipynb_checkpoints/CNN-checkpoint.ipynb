{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN\n",
    "\n",
    "### project goals:\n",
    "\n",
    "1. read data -- Done\n",
    "2. prepossing pictures -- Done\n",
    "3. model structure\n",
    "4. generate features\n",
    "\n",
    "python2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import cv2 \n",
    "#can be installed by running \"!pip install opencv-python\"\n",
    "#in current .ipynb\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from datetime import timedelta\n",
    "import tempfile\n",
    "import math\n",
    "import random\n",
    "#Adding Seed so that random initialization is consistent\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_classes = 3\n",
    "image_size = 128\n",
    "validation_size = 0.2\n",
    "num_channels = 3\n",
    "batch_size = 64\n",
    "NUM_SAME_PIC = 3\n",
    "learning_rate = 0.001 \n",
    "train_path = \"../data/training_set/train/\"\n",
    "train_class_path = \"../data/training_set/label_train.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reading pictures and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataSet(object):\n",
    "\n",
    "    def __init__(self, images, labels, cls):\n",
    "        self._num_examples = images.shape[0]\n",
    "\n",
    "        self._images = images\n",
    "        self._labels = labels\n",
    "        self._cls = cls\n",
    "        self._epochs_done = 0\n",
    "        self._index_in_epoch = 0\n",
    "\n",
    "    @property\n",
    "    def images(self):\n",
    "        return self._images\n",
    "    \n",
    "    @property\n",
    "    def labels(self):\n",
    "        return self._labels\n",
    "    \n",
    "    @property\n",
    "    def cls(self):\n",
    "        return self._cls\n",
    "\n",
    "    @property\n",
    "    def num_examples(self):\n",
    "        return self._num_examples\n",
    "\n",
    "    @property\n",
    "    def epochs_done(self):\n",
    "        return self._epochs_done\n",
    "\n",
    "    def next_batch(self, batch_size):\n",
    "        \"\"\"Return the next `batch_size` examples from this data set.\"\"\"\n",
    "        start = self._index_in_epoch\n",
    "        self._index_in_epoch += batch_size\n",
    "\n",
    "        if self._index_in_epoch > self._num_examples:\n",
    "            # After each epoch we update this\n",
    "            self._epochs_done += 1\n",
    "            start = 0\n",
    "            self._index_in_epoch = batch_size\n",
    "            assert batch_size <= self._num_examples\n",
    "        end = self._index_in_epoch\n",
    "\n",
    "        return self._images[start:end], self._labels[start:end], self._cls[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# images including all the pictures\n",
    "def read_train_sets(train_path, train_class_path, image_size, validation_size):\n",
    "    class DataSets(object):\n",
    "        pass\n",
    "    data_sets = DataSets()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    #==== reading pictures =====\n",
    "    files = [train_path + f for f in os.listdir(train_path) if f.endswith('.jpg')]\n",
    "    images = []\n",
    "    for fl in files:\n",
    "        image = cv2.imread(fl)\n",
    "        image = cv2.resize(image, (image_size, image_size),0,0, cv2.INTER_LINEAR)\n",
    "        image = image.astype(np.float32)\n",
    "        image = np.multiply(image, 1.0 / 255.0)\n",
    "        images.append(image)\n",
    "        \n",
    "        #flip\n",
    "        image_flip = cv2.flip(image,1)\n",
    "        images.append(image_flip)\n",
    "        \n",
    "        #rotate by -15 ~ +15 degree\n",
    "        Left = np.random.uniform(-15, 15)\n",
    "        rows, cols, color = image.shape\n",
    "        M = cv2.getRotationMatrix2D((cols/2, rows/2), Left, 1)\n",
    "        image_rotate = cv2.warpAffine(image, M, (cols, rows))\n",
    "        images.append(image_rotate)\n",
    "    \n",
    "    print(\"--- reading image part-I DONE %s seconds ---\" % (time.time() - start_time))\n",
    "    images = np.array(images)\n",
    "\n",
    "    print(\"--- reading image all DONE %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "    \n",
    "    #==== reading classes =====\n",
    "    cls = []\n",
    "    clsFile = pd.read_csv(train_class_path, index_col=0)\n",
    "    cls_tmp = clsFile.iloc[:,0].values\n",
    "    for cur in cls_tmp:\n",
    "        cls += [cur]*NUM_SAME_PIC\n",
    "    cls = np.array(cls)\n",
    "\n",
    "    print(\"--- reading classes DONE %s seconds ---\" % (time.time() - start_time))\n",
    "    \n",
    "    print \"Is Label Number = Image Number?\", cls.shape[0] == images.shape[0]\n",
    "\n",
    "    #==== adding labels =====\n",
    "    labels = []\n",
    "    for i in cls:\n",
    "        label = np.zeros(num_classes)\n",
    "        label[i] = 1.0\n",
    "        labels.append(label)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    \n",
    "    #==== sampling validation out =====\n",
    "    if isinstance(validation_size, float):\n",
    "        validation_size = int(validation_size * (images.shape[0]/NUM_SAME_PIC))\n",
    "    shuffle_idx_tmp = np.random.choice(images.shape[0]/NUM_SAME_PIC, validation_size, replace=False) #3000\n",
    "    tmp = np.asarray(range(images.shape[0])) #9000\n",
    "    shuffle_idx = np.reshape([tmp[i * NUM_SAME_PIC : (i+1) * NUM_SAME_PIC] for i in shuffle_idx_tmp], (3*len(shuffle_idx_tmp))) \n",
    "    # for [0, 7] generate [0, 1, 2, 21, 22, 23]\n",
    "    not_shuffle_indx = [x for x in tmp if x not in shuffle_idx]\n",
    "  \n",
    "    validation_images = images[shuffle_idx]\n",
    "    validation_labels = labels[shuffle_idx]\n",
    "    validation_cls = cls[shuffle_idx]\n",
    "    \n",
    "    train_images = images[not_shuffle_indx]\n",
    "    train_labels = labels[not_shuffle_indx]\n",
    "    train_cls = cls[not_shuffle_indx]\n",
    "    \n",
    "    train_images, train_labels, train_cls = shuffle(train_images, train_labels, train_cls)\n",
    "    \n",
    "    print(\"--- all DONE %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "\n",
    "    data_sets.train = DataSet(train_images, train_labels, train_cls)\n",
    "    data_sets.valid = DataSet(validation_images, validation_labels, validation_cls)\n",
    "\n",
    "    return data_sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- reading image part-I DONE 49.3510489464 seconds ---\n",
      "--- reading image all DONE 69.8219430447 seconds ---\n",
      "--- reading classes DONE 70.2588789463 seconds ---\n",
      "Is Label Number = Image Number? True\n",
      "--- all DONE 233.32565403 seconds ---\n",
      "Complete reading input data. Will Now print a snippet of it\n",
      "Number of files in Training-set:\t\t7200\n",
      "Number of files in Validation-set:\t1800\n"
     ]
    }
   ],
   "source": [
    "data = read_train_sets(train_path, train_class_path, image_size, validation_size=validation_size)\n",
    "print(\"Complete reading input data. Will Now print a snippet of it\")\n",
    "print(\"Number of files in Training-set:\\t\\t{}\".format(len(data.train.cls)))\n",
    "print(\"Number of files in Validation-set:\\t{}\".format(len(data.valid.cls)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholders and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "session = tf.Session()\n",
    "x = tf.placeholder(tf.float32, shape=[None, image_size,image_size,num_channels], name='x')\n",
    "y_true = tf.placeholder(tf.float32, shape=[None, num_classes], name='y_true') #labels\n",
    "y_true_cls = tf.argmax(y_true, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Network graph params\n",
    "filter_size_conv1 = 3 \n",
    "num_filters_conv1 = 32\n",
    "\n",
    "filter_size_conv2 = 3\n",
    "num_filters_conv2 = 32\n",
    "\n",
    "filter_size_conv3 = 3\n",
    "num_filters_conv3 = 64\n",
    "    \n",
    "fc_layer_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Layers Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_weights(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.05))\n",
    "\n",
    "def create_biases(size):\n",
    "    return tf.Variable(tf.constant(0.05, shape=[size]))\n",
    "\n",
    "def create_convolutional_layer(input,\n",
    "               num_input_channels, \n",
    "               conv_filter_size,        \n",
    "               num_filters,\n",
    "               layer_name):  \n",
    "    \n",
    "    ## We shall define the weights that will be trained using create_weights function.\n",
    "    weights = create_weights(shape=[conv_filter_size, conv_filter_size, num_input_channels, num_filters])\n",
    "    ## We create biases using the create_biases function. These are also trained.\n",
    "    biases = create_biases(num_filters)\n",
    "\n",
    "    ## Creating the convolutional layer\n",
    "    layer = tf.nn.conv2d(input=input,\n",
    "                     filter=weights,\n",
    "                     strides=[1, 1, 1, 1],\n",
    "                     padding='SAME', name = layer_name+'_conv2d')\n",
    "\n",
    "    layer += biases\n",
    "\n",
    "    ## We shall be using max-pooling.  \n",
    "    layer = tf.nn.max_pool(value=layer,\n",
    "                            ksize=[1, 2, 2, 1],\n",
    "                            strides=[1, 2, 2, 1],\n",
    "                            padding='SAME',name = layer_name+'_max_pool')\n",
    "    ## Output of pooling is fed to Relu which is the activation function for us.\n",
    "    layer = tf.nn.relu(layer, name = layer_name+'_relu')\n",
    "\n",
    "    return layer\n",
    "\n",
    "    \n",
    "\n",
    "def create_flatten_layer(layer):\n",
    "    #We know that the shape of the layer will be [batch_size img_size img_size num_channels] \n",
    "    # But let's get it from the previous layer.\n",
    "    layer_shape = layer.get_shape()\n",
    "\n",
    "    ## Number of features will be img_height * img_width* num_channels. But we shall calculate it in place of hard-coding it.\n",
    "    num_features = layer_shape[1:4].num_elements()\n",
    "\n",
    "    ## Now, we Flatten the layer so we shall have to reshape to num_features\n",
    "    layer = tf.reshape(layer, [-1, num_features])\n",
    "\n",
    "    return layer\n",
    "\n",
    "\n",
    "def create_fc_layer(input, num_inputs, num_outputs, layer_name, use_relu=True,):\n",
    "    \n",
    "    #Let's define trainable weights and biases.\n",
    "    weights = create_weights(shape=[num_inputs, num_outputs])\n",
    "    biases = create_biases(num_outputs)\n",
    "\n",
    "    # Fully connected layer takes input x and produces wx+b.Since, these are matrices, we use matmul function in Tensorflow\n",
    "    layer = tf.matmul(input, weights) + biases\n",
    "    if use_relu:\n",
    "        layer = tf.nn.relu(layer, name = layer_name)\n",
    "\n",
    "    return layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "layer_conv1 = create_convolutional_layer(input=x,\n",
    "               num_input_channels=num_channels,\n",
    "               conv_filter_size=filter_size_conv1,\n",
    "               num_filters=num_filters_conv1,\n",
    "               layer_name = 'layer_conv1')\n",
    "layer_conv2 = create_convolutional_layer(input=layer_conv1,\n",
    "               num_input_channels=num_filters_conv1,\n",
    "               conv_filter_size=filter_size_conv2,\n",
    "               num_filters=num_filters_conv2,\n",
    "               layer_name = 'layer_conv2')\n",
    "\n",
    "layer_conv3 = create_convolutional_layer(input=layer_conv2,\n",
    "               num_input_channels=num_filters_conv2,\n",
    "               conv_filter_size=filter_size_conv3,\n",
    "               num_filters=num_filters_conv3,\n",
    "               layer_name = 'layer_conv3')\n",
    "          \n",
    "layer_flat = create_flatten_layer(layer_conv3)\n",
    "\n",
    "layer_fc1 = create_fc_layer(input=layer_flat,\n",
    "                     num_inputs=layer_flat.get_shape()[1:4].num_elements(),\n",
    "                     num_outputs=fc_layer_size,\n",
    "                     layer_name = 'layer_fc1',\n",
    "                     use_relu = True)\n",
    "\n",
    "layer_fc2 = create_fc_layer(input=layer_fc1,\n",
    "                     num_inputs=fc_layer_size,\n",
    "                     num_outputs=num_classes,\n",
    "                     layer_name = 'layer_fc2',\n",
    "                     use_relu = False) \n",
    "\n",
    "y_pred = tf.nn.softmax(layer_fc2,name='y_pred')\n",
    "\n",
    "y_pred_cls = tf.argmax(y_pred, axis=1)\n",
    "\n",
    "\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=layer_fc2,\n",
    "                                                    labels=y_true)\n",
    "cost = tf.reduce_mean(cross_entropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "correct_prediction = tf.equal(y_pred_cls, y_true_cls)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Function and Saving to Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(num_iteration):\n",
    "    record_best_acc = 0\n",
    "    record_best_val_acc = 0\n",
    "    \n",
    "    training_summary = tf.summary.scalar(\"training_accuracy\", accuracy)\n",
    "    validation_summary = tf.summary.scalar(\"validation_accuracy\", accuracy)\n",
    "    validation_cost = tf.summary.scalar(\"validation_cost\", cost)\n",
    "    \n",
    "    graph_location = tempfile.mkdtemp()\n",
    "    print('Saving graph to: %s' % graph_location)\n",
    "    train_writer = tf.summary.FileWriter(graph_location)\n",
    "    train_writer.add_graph(tf.get_default_graph())\n",
    "    \n",
    "    writer = tf.summary.FileWriter(graph_location, graph=tf.get_default_graph())\n",
    "    \n",
    "    for i in range(num_iteration):\n",
    "\n",
    "        x_batch, y_true_batch, cls_batch = data.train.next_batch(batch_size)\n",
    "        x_valid_batch, y_valid_batch, valid_cls_batch = data.valid.next_batch(batch_size)\n",
    "\n",
    "        \n",
    "        feed_dict_tr = {x: x_batch,\n",
    "                           y_true: y_true_batch}\n",
    "        feed_dict_val = {x: x_valid_batch,\n",
    "                              y_true: y_valid_batch}\n",
    "\n",
    "        session.run(optimizer, feed_dict=feed_dict_tr)\n",
    "        \n",
    "        \n",
    "        if i % 200 == 0: \n",
    "            epoch = int(i / 100)\n",
    "            \n",
    "            #tensorboard\n",
    "            acc,train_summ = session.run([accuracy,training_summary], feed_dict=feed_dict_tr)\n",
    "            writer.add_summary(train_summ, i)\n",
    "            val_acc,val_loss, valid_summ,val_cost = session.run([accuracy, cost, validation_summary,validation_cost], feed_dict=feed_dict_val)\n",
    "            writer.add_summary(valid_summ, i)\n",
    "            writer.add_summary(val_cost, i)\n",
    "            \n",
    "            # print output\n",
    "            #print(val_loss)\n",
    "            msg = \"Training Epoch {0} --- Training Accuracy: {1:>6.1%}, Validation Accuracy: {2:>6.1%},  Validation Loss: {3:.3f}\"\n",
    "            print(msg.format(epoch + 1, acc, val_acc, val_loss))\n",
    "            \n",
    "            if (val_acc > record_best_val_acc) and (acc > record_best_acc) :\n",
    "                record_best_acc = acc\n",
    "                record_best_val_acc = val_acc\n",
    "                \n",
    "                saver.save(session, 'where_are_my_puppies') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving graph to: /var/folders/1v/lv_zkyg934zczh6y3zwd76m00000gn/T/tmpi63tjb\n",
      "Training Epoch 1 --- Training Accuracy:  40.6%, Validation Accuracy:  20.3%,  Validation Loss: 2.271\n",
      "Training Epoch 3 --- Training Accuracy:  71.9%, Validation Accuracy:  76.6%,  Validation Loss: 0.617\n",
      "Training Epoch 5 --- Training Accuracy:  79.7%, Validation Accuracy:  82.8%,  Validation Loss: 0.533\n",
      "Training Epoch 7 --- Training Accuracy:  87.5%, Validation Accuracy:  70.3%,  Validation Loss: 0.708\n",
      "Training Epoch 9 --- Training Accuracy:  93.8%, Validation Accuracy:  79.7%,  Validation Loss: 0.401\n",
      "Training Epoch 11 --- Training Accuracy:  95.3%, Validation Accuracy:  70.3%,  Validation Loss: 0.839\n",
      "Training Epoch 13 --- Training Accuracy:  98.4%, Validation Accuracy:  84.4%,  Validation Loss: 0.484\n",
      "Training Epoch 15 --- Training Accuracy: 100.0%, Validation Accuracy:  78.1%,  Validation Loss: 1.056\n",
      "Training Epoch 17 --- Training Accuracy: 100.0%, Validation Accuracy:  90.6%,  Validation Loss: 0.274\n",
      "Training Epoch 19 --- Training Accuracy: 100.0%, Validation Accuracy:  65.6%,  Validation Loss: 2.847\n",
      "Training Epoch 21 --- Training Accuracy: 100.0%, Validation Accuracy:  76.6%,  Validation Loss: 1.122\n",
      "Training Epoch 23 --- Training Accuracy: 100.0%, Validation Accuracy:  93.8%,  Validation Loss: 0.594\n",
      "Training Epoch 25 --- Training Accuracy: 100.0%, Validation Accuracy:  87.5%,  Validation Loss: 0.453\n",
      "Training Epoch 27 --- Training Accuracy: 100.0%, Validation Accuracy:  89.1%,  Validation Loss: 0.546\n",
      "Training Epoch 29 --- Training Accuracy: 100.0%, Validation Accuracy:  79.7%,  Validation Loss: 1.154\n",
      "Training Epoch 31 --- Training Accuracy: 100.0%, Validation Accuracy:  89.1%,  Validation Loss: 0.612\n",
      "Training Epoch 33 --- Training Accuracy: 100.0%, Validation Accuracy:  65.6%,  Validation Loss: 3.664\n",
      "Training Epoch 35 --- Training Accuracy: 100.0%, Validation Accuracy:  82.8%,  Validation Loss: 1.399\n",
      "Training Epoch 37 --- Training Accuracy: 100.0%, Validation Accuracy:  93.8%,  Validation Loss: 0.744\n",
      "Training Epoch 39 --- Training Accuracy: 100.0%, Validation Accuracy:  85.9%,  Validation Loss: 0.572\n",
      "Training Epoch 41 --- Training Accuracy: 100.0%, Validation Accuracy:  89.1%,  Validation Loss: 0.622\n",
      "Training Epoch 43 --- Training Accuracy: 100.0%, Validation Accuracy:  79.7%,  Validation Loss: 1.364\n",
      "Training Epoch 45 --- Training Accuracy: 100.0%, Validation Accuracy:  89.1%,  Validation Loss: 0.661\n",
      "Training Epoch 47 --- Training Accuracy: 100.0%, Validation Accuracy:  67.2%,  Validation Loss: 4.127\n",
      "Training Epoch 49 --- Training Accuracy: 100.0%, Validation Accuracy:  82.8%,  Validation Loss: 1.459\n",
      "Training Epoch 51 --- Training Accuracy: 100.0%, Validation Accuracy:  93.8%,  Validation Loss: 0.823\n",
      "Training Epoch 53 --- Training Accuracy: 100.0%, Validation Accuracy:  84.4%,  Validation Loss: 0.627\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-41daa521371a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_iteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-d87c19f5f30c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(num_iteration)\u001b[0m\n\u001b[1;32m     25\u001b[0m                               y_true: y_valid_batch}\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/laohuang/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/laohuang/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/laohuang/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/laohuang/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/laohuang/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "session.run(tf.global_variables_initializer()) \n",
    "saver = tf.train.Saver()\n",
    "train(num_iteration=10000)\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sess' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-8bf3ecff1273>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sess' is not defined"
     ]
    }
   ],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
