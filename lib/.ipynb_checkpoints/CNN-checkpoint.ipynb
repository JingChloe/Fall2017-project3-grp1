{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN\n",
    "\n",
    "### project goals:\n",
    "\n",
    "1. read data -- Done\n",
    "2. prepossing pictures -- Done\n",
    "3. model structure\n",
    "4. generate features\n",
    "\n",
    "python2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import cv2 \n",
    "#can be installed by running \"!pip install opencv-python\"\n",
    "#in current .ipynb\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from datetime import timedelta\n",
    "import math\n",
    "import random\n",
    "#Adding Seed so that random initialization is consistent\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_classes = 3\n",
    "image_size = 128\n",
    "validation_size = 0.1\n",
    "num_channels = 3\n",
    "batch_size = 32\n",
    "total_iterations = 0\n",
    "NUM_SAME_PIC = 2\n",
    "train_path = \"../data/training_set/train/\"\n",
    "train_class_path = \"../data/training_set/label_train.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reading pictures and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataSet(object):\n",
    "\n",
    "    def __init__(self, images, labels, cls):\n",
    "        self._num_examples = images.shape[0]\n",
    "\n",
    "        self._images = images\n",
    "        self._labels = labels\n",
    "        self._cls = cls\n",
    "        self._epochs_done = 0\n",
    "        self._index_in_epoch = 0\n",
    "\n",
    "    @property\n",
    "    def images(self):\n",
    "        return self._images\n",
    "    \n",
    "    @property\n",
    "    def labels(self):\n",
    "        return self._labels\n",
    "    \n",
    "    @property\n",
    "    def cls(self):\n",
    "        return self._cls\n",
    "\n",
    "    @property\n",
    "    def num_examples(self):\n",
    "        return self._num_examples\n",
    "\n",
    "    @property\n",
    "    def epochs_done(self):\n",
    "        return self._epochs_done\n",
    "\n",
    "    def next_batch(self, batch_size):\n",
    "        \"\"\"Return the next `batch_size` examples from this data set.\"\"\"\n",
    "        start = self._index_in_epoch\n",
    "        self._index_in_epoch += batch_size\n",
    "\n",
    "        if self._index_in_epoch > self._num_examples:\n",
    "            # After each epoch we update this\n",
    "            self._epochs_done += 1\n",
    "            start = 0\n",
    "            self._index_in_epoch = batch_size\n",
    "            assert batch_size <= self._num_examples\n",
    "        end = self._index_in_epoch\n",
    "\n",
    "        return self._images[start:end], self._labels[start:end], self._cls[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# images including all the pictures\n",
    "def read_train_sets(train_path, train_class_path, image_size, validation_size):\n",
    "    class DataSets(object):\n",
    "        pass\n",
    "    data_sets = DataSets()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    #==== reading pictures =====\n",
    "    files = [train_path + f for f in os.listdir(train_path) if f.endswith('.jpg')]\n",
    "    images = []\n",
    "    for fl in files:\n",
    "        image = cv2.imread(fl)\n",
    "        image = cv2.resize(image, (image_size, image_size),0,0, cv2.INTER_LINEAR)\n",
    "        image = image.astype(np.float32)\n",
    "        image = np.multiply(image, 1.0 / 255.0)\n",
    "        images.append(image)\n",
    "        \n",
    "        #flip\n",
    "        image_flip = cv2.flip(image,1)\n",
    "        images.append(image_flip)\n",
    "        \n",
    "        #rotate by -15 ~ +15 degree\n",
    "        #Left = np.random.uniform(-15, 15)\n",
    "        #rows, cols, color = image.shape\n",
    "        #M = cv2.getRotationMatrix2D((cols/2, rows/2), Left, 1)\n",
    "        #image_rotate = cv2.warpAffine(image, M, (cols, rows))\n",
    "        #images.append(image_rotate)\n",
    "        \n",
    "    images = np.array(images)\n",
    "\n",
    "    #cv2.imshow(\"image\", images[15])\n",
    "    print(\"--- reading image DONE %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "    \n",
    "    #==== reading classes =====\n",
    "    cls = []\n",
    "    clsFile = pd.read_csv(train_class_path, index_col=0)\n",
    "    cls_tmp = clsFile.iloc[:,0].values\n",
    "    for cur in cls_tmp:\n",
    "        cls += [cur]*NUM_SAME_PIC\n",
    "    cls = np.array(cls)\n",
    "\n",
    "    print(\"--- reading classes DONE %s seconds ---\" % (time.time() - start_time))\n",
    "    \n",
    "    print \"Is Label Number = Image Number?\", cls.shape[0] == images.shape[0]\n",
    "    \n",
    "    \n",
    "    \n",
    "    #==== adding labels =====\n",
    "    labels = []\n",
    "    for i in cls:\n",
    "        label = np.zeros(num_classes)\n",
    "        label[i] = 1.0\n",
    "        labels.append(label)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    \n",
    "    #images, cls = load_train(train_path, classes) see above\n",
    "    images, labels, cls = shuffle(images, labels, cls)  \n",
    "\n",
    "    if isinstance(validation_size, float):\n",
    "        validation_size = int(validation_size * images.shape[0])\n",
    "\n",
    "    validation_images = images[:validation_size]\n",
    "    validation_labels = labels[:validation_size]\n",
    "    validation_cls = cls[:validation_size]\n",
    "\n",
    "    train_images = images[validation_size:]\n",
    "    train_labels = labels[validation_size:]\n",
    "    train_cls = cls[validation_size:]\n",
    "\n",
    "    data_sets.train = DataSet(train_images, train_labels, train_cls)\n",
    "    data_sets.valid = DataSet(validation_images, validation_labels, validation_cls)\n",
    "\n",
    "    return data_sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- reading image DONE 43.7131609917 seconds ---\n",
      "--- reading classes DONE 43.8475880623 seconds ---\n",
      "Is Label Number = Image Number? True\n",
      "Complete reading input data. Will Now print a snippet of it\n",
      "Number of files in Training-set:\t\t5400\n",
      "Number of files in Validation-set:\t600\n"
     ]
    }
   ],
   "source": [
    "data = read_train_sets(train_path, train_class_path, image_size, validation_size=validation_size)\n",
    "print(\"Complete reading input data. Will Now print a snippet of it\")\n",
    "print(\"Number of files in Training-set:\\t\\t{}\".format(len(data.train.cls)))\n",
    "print(\"Number of files in Validation-set:\\t{}\".format(len(data.valid.cls)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholders and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "session = tf.Session()\n",
    "x = tf.placeholder(tf.float32, shape=[None, image_size,image_size,num_channels], name='x')\n",
    "y_true = tf.placeholder(tf.float32, shape=[None, num_classes], name='y_true') #labels\n",
    "y_true_cls = tf.argmax(y_true, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Network graph params\n",
    "filter_size_conv1 = 3 \n",
    "num_filters_conv1 = 32\n",
    "\n",
    "filter_size_conv2 = 3\n",
    "num_filters_conv2 = 32\n",
    "\n",
    "filter_size_conv3 = 3\n",
    "num_filters_conv3 = 64\n",
    "    \n",
    "fc_layer_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN layers definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "non-default argument follows default argument (<ipython-input-13-63d773c2c6a7>, line 52)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-63d773c2c6a7>\"\u001b[0;36m, line \u001b[0;32m52\u001b[0m\n\u001b[0;31m    def create_fc_layer(input, num_inputs, num_outputs,use_relu=True,layer_name):\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m non-default argument follows default argument\n"
     ]
    }
   ],
   "source": [
    "def create_weights(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.05))\n",
    "\n",
    "def create_biases(size):\n",
    "    return tf.Variable(tf.constant(0.05, shape=[size]))\n",
    "\n",
    "def create_convolutional_layer(input,\n",
    "               num_input_channels, \n",
    "               conv_filter_size,        \n",
    "               num_filters,\n",
    "               layer_name):  \n",
    "    \n",
    "    ## We shall define the weights that will be trained using create_weights function.\n",
    "    weights = create_weights(shape=[conv_filter_size, conv_filter_size, num_input_channels, num_filters])\n",
    "    ## We create biases using the create_biases function. These are also trained.\n",
    "    biases = create_biases(num_filters)\n",
    "\n",
    "    ## Creating the convolutional layer\n",
    "    layer = tf.nn.conv2d(input=input,\n",
    "                     filter=weights,\n",
    "                     strides=[1, 1, 1, 1],\n",
    "                     padding='SAME', name = layer_name+'_conv2d')\n",
    "\n",
    "    layer += biases\n",
    "\n",
    "    ## We shall be using max-pooling.  \n",
    "    layer = tf.nn.max_pool(value=layer,\n",
    "                            ksize=[1, 2, 2, 1],\n",
    "                            strides=[1, 2, 2, 1],\n",
    "                            padding='SAME',name = layer_name+'_max_pool')\n",
    "    ## Output of pooling is fed to Relu which is the activation function for us.\n",
    "    layer = tf.nn.relu(layer, name = layer_name+'_relu')\n",
    "\n",
    "    return layer\n",
    "\n",
    "    \n",
    "\n",
    "def create_flatten_layer(layer):\n",
    "    #We know that the shape of the layer will be [batch_size img_size img_size num_channels] \n",
    "    # But let's get it from the previous layer.\n",
    "    layer_shape = layer.get_shape()\n",
    "\n",
    "    ## Number of features will be img_height * img_width* num_channels. But we shall calculate it in place of hard-coding it.\n",
    "    num_features = layer_shape[1:4].num_elements()\n",
    "\n",
    "    ## Now, we Flatten the layer so we shall have to reshape to num_features\n",
    "    layer = tf.reshape(layer, [-1, num_features])\n",
    "\n",
    "    return layer\n",
    "\n",
    "\n",
    "def create_fc_layer(input, num_inputs, num_outputs, layer_name, use_relu=True,):\n",
    "    \n",
    "    #Let's define trainable weights and biases.\n",
    "    weights = create_weights(shape=[num_inputs, num_outputs])\n",
    "    biases = create_biases(num_outputs)\n",
    "\n",
    "    # Fully connected layer takes input x and produces wx+b.Since, these are matrices, we use matmul function in Tensorflow\n",
    "    layer = tf.matmul(input, weights) + biases\n",
    "    if use_relu:\n",
    "        layer = tf.nn.relu(layer, name = layer_name)\n",
    "\n",
    "    return layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_convolutional_layer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-7aef9c45656a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m layer_conv1 = create_convolutional_layer(input=x,\n\u001b[0m\u001b[1;32m      2\u001b[0m                \u001b[0mnum_input_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_channels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                \u001b[0mconv_filter_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilter_size_conv1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                \u001b[0mnum_filters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_filters_conv1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                layer_name = 'layer_conv1')\n",
      "\u001b[0;31mNameError\u001b[0m: name 'create_convolutional_layer' is not defined"
     ]
    }
   ],
   "source": [
    "layer_conv1 = create_convolutional_layer(input=x,\n",
    "               num_input_channels=num_channels,\n",
    "               conv_filter_size=filter_size_conv1,\n",
    "               num_filters=num_filters_conv1,\n",
    "               layer_name = 'layer_conv1')\n",
    "layer_conv2 = create_convolutional_layer(input=layer_conv1,\n",
    "               num_input_channels=num_filters_conv1,\n",
    "               conv_filter_size=filter_size_conv2,\n",
    "               num_filters=num_filters_conv2,\n",
    "               layer_name = 'layer_conv2')\n",
    "\n",
    "layer_conv3 = create_convolutional_layer(input=layer_conv2,\n",
    "               num_input_channels=num_filters_conv2,\n",
    "               conv_filter_size=filter_size_conv3,\n",
    "               num_filters=num_filters_conv3,\n",
    "               layer_name = 'layer_conv3')\n",
    "          \n",
    "layer_flat = create_flatten_layer(layer_conv3)\n",
    "\n",
    "layer_fc1 = create_fc_layer(input=layer_flat,\n",
    "                     num_inputs=layer_flat.get_shape()[1:4].num_elements(),\n",
    "                     num_outputs=fc_layer_size,\n",
    "                     layer_name = 'layer_fc1',\n",
    "                     use_relu=True)\n",
    "\n",
    "layer_fc2 = create_fc_layer(input=layer_fc1,\n",
    "                     num_inputs=fc_layer_size,\n",
    "                     num_outputs=num_classes,\n",
    "                     ayer_name = 'layer_fc2'\n",
    "                     use_relu=False) \n",
    "\n",
    "y_pred = tf.nn.softmax(layer_fc2,name='y_pred')\n",
    "\n",
    "y_pred_cls = tf.argmax(y_pred, axis=1)\n",
    "\n",
    "\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=layer_fc2,\n",
    "                                                    labels=y_true)\n",
    "cost = tf.reduce_mean(cross_entropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(cost)\n",
    "correct_prediction = tf.equal(y_pred_cls, y_true_cls)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_progress(epoch, feed_dict_train, feed_dict_validate, val_loss):\n",
    "    acc = session.run(accuracy, feed_dict=feed_dict_train)\n",
    "    val_acc = session.run(accuracy, feed_dict=feed_dict_validate)\n",
    "    msg = \"Training Epoch {0} --- Training Accuracy: {1:>6.1%}, Validation Accuracy: {2:>6.1%},  Validation Loss: {3:.3f}\"\n",
    "    print(msg.format(epoch + 1, acc, val_acc, val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(num_iteration):\n",
    "    global total_iterations\n",
    "    for i in range(total_iterations,\n",
    "                   total_iterations + num_iteration):\n",
    "\n",
    "        x_batch, y_true_batch, cls_batch = data.train.next_batch(batch_size)\n",
    "        x_valid_batch, y_valid_batch, valid_cls_batch = data.valid.next_batch(batch_size)\n",
    "\n",
    "        \n",
    "        feed_dict_tr = {x: x_batch,\n",
    "                           y_true: y_true_batch}\n",
    "        feed_dict_val = {x: x_valid_batch,\n",
    "                              y_true: y_valid_batch}\n",
    "\n",
    "        session.run(optimizer, feed_dict=feed_dict_tr)\n",
    "\n",
    "        if i % int(data.train.num_examples/batch_size) == 0: \n",
    "        #if i % 100 == 0:\n",
    "            val_loss = session.run(cost, feed_dict=feed_dict_val)\n",
    "            epoch = int(i / int(data.train.num_examples/batch_size))    \n",
    "            show_progress(epoch, feed_dict_tr, feed_dict_val, val_loss)\n",
    "            saver.save(session, 'where_are_my_puppies') \n",
    "\n",
    "\n",
    "    total_iterations += num_iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_iterations = 0\n",
    "session.run(tf.global_variables_initializer()) \n",
    "total_iterations = 0\n",
    "saver = tf.train.Saver()\n",
    "train(num_iteration=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
